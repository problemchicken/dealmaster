# DealMaster React Native App

A React Native starter project bootstrapped with TypeScript that showcases a simple authentication flow powered by Zustand and Axios. The application includes Login, Home, and Settings screens connected with React Navigation.

## Features

- ğŸš€ **React Native 0.72** with TypeScript and ESLint configuration
- ğŸ” **Authentication state** handled via [Zustand](https://github.com/pmndrs/zustand)
- ğŸŒ **Axios HTTP client** with an interceptor that injects auth tokens
- ğŸ§­ **React Navigation** native stack navigator (Login â†’ Home â†’ Settings)
- ğŸ¨ Shared design tokens and reusable UI components

## Project Structure

```
.
â”œâ”€â”€ App.tsx
â”œâ”€â”€ index.js
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ components
â”‚   â”œâ”€â”€ navigation
â”‚   â”œâ”€â”€ screens
â”‚   â”œâ”€â”€ services
â”‚   â”œâ”€â”€ store
â”‚   â””â”€â”€ theme
â””â”€â”€ ...
```

## Getting Started

1. Install dependencies:

   ```bash
   npm install
   ```

2. Start the Metro bundler:

   ```bash
   npm run start
   ```

3. Run the application:

   ```bash
   # For Android
   npm run android

   # For iOS (requires macOS)
   npm run ios
   ```

## Speech-to-Text Input

The shared speech-to-text bridge lives in [`src/services/speech.ts`](src/services/speech.ts) and exposes helper methods for opening the microphone, handling partial results, and submitting transcripts. A minimal usage example:

```ts
import {addSpeechListener, open, stop, send} from '../services/speech';

const subscription = addSpeechListener('stt_final', payload => {
  // Forward the final transcript to your chat flow.
  send(payload.text);
});

await open();
const finalText = await stop();
subscription();
```

### Android-specific setup

- Declare the microphone permission by adding `<uses-permission android:name="android.permission.RECORD_AUDIO" />` to `android/app/src/main/AndroidManifest.xml`.
- Register the native package in `MainApplication`:

  ```kotlin
  override fun getPackages(): List<ReactPackage> =
    listOf(
      MainReactPackage(),
      SpeechPackage(),
    )
  ```

- Call `requestPermission()` before recording so the module can prompt users at runtime. When a user permanently denies access the module emits the `stt_permission_denied` telemetry event and resolves with a `blocked` status.

### iOS-specific setup

- Expo apps automatically merge the `ios.infoPlist` entries declared in [`app.json`](app.json); ensure the microphone and speech recognition usage descriptions are present before submitting to the App Store.
- No extra React Native registration is required â€” the Swift bridge is exported through `SpeechModuleBridge.m` and is available as `NativeModules.SpeechModule`.
- For local development run `npx pod-install` after installing JavaScript dependencies so the native module is compiled into the iOS project.

### æœ¬æ©Ÿä¸€éµå•Ÿå‹• iOS æ¨¡æ“¬å™¨

```bash
chmod +x scripts/run_ios_sim.sh
./scripts/run_ios_sim.sh
```

> å‚™è¨»ï¼šé¦–æ¬¡ç·¨è­¯éœ€è¦è¼ƒé•·æ™‚é–“å±¬æ­£å¸¸ç¾è±¡ï¼›è‹¥æ©Ÿå™¨æ²’æœ‰ iPhone 17 Pro æ¨¡æ“¬å™¨ï¼Œè…³æœ¬æœƒè‡ªå‹•å›é€€åˆ°ç¾æœ‰å¯ç”¨çš„è£ç½®é¡å‹ã€‚

### Testing notes

- **Permissions** â€“ Use `requestPermission()` to trigger the native prompt. On the iOS Simulator you can script responses with:

  ```bash
  xcrun simctl privacy booted speech-recognition grant com.dealmaster
  xcrun simctl privacy booted speech-recognition deny com.dealmaster
  xcrun simctl privacy booted microphone grant com.dealmaster
  xcrun simctl privacy booted microphone deny com.dealmaster
  ```

- **Microphone input** â€“ Feed canned audio to the simulator microphone while testing partial/final eventsã€‚å…ˆä»¥ `node scripts/make-sample-wav.js` ç”¢ç”Ÿ `tmp/sample-command.wav`ï¼š

  ```bash
  node scripts/make-sample-wav.js tmp/sample-command.wav
  xcrun simctl io booted microphone ./tmp/sample-command.wav
  ```

  End the stream with `Ctrl+C` once you observe `stt_final` firing.

- **Negotiation pipeline** â€“ Use `submitSpeechNegotiationSample()` from [`src/services/speechNegotiation.ts`](src/services/speechNegotiation.ts) to forward captured audio (Base64) to your backend `/api/speech-endpoint`. The helper automatically records end-to-end latency and error rate telemetry via the new `speech_pipeline_complete` event.

- **Telemetry expectations** â€“ Example payloads emitted via `trackSttEvent`:

  ```ts
  // stt_partial
  {
    platform: 'ios',
    provider: 'native',
    sequence_id: 1,
    text_length: 12,
    partial_transcript: 'turn on lights',
  }

  // stt_final
  {
    platform: 'ios',
    provider: 'native',
    duration_ms: 3250,
    text_length: 12,
    transcript: 'turn on lights',
  }

  // stt_error
  {
    platform: 'ios',
    provider: 'native',
    error_code: 'no_speech_detected',
    message: 'SFSpeechErrorCode.noSpeech',
    native_flag: true,
  }

  // stt_permission_denied
  {
    platform: 'ios',
    provider: 'native',
    error_code: 'permission_denied',
    native_flag: true,
  }
  ```

## Environment Variables

Copy `.env.example` to `.env` and update the `API_URL` value to point to your backend service. The value will be used when creating the Axios instance.

For native OCR flagã€æ¬Šé™èˆ‡æœ¬æ©ŸåŸ·è¡Œèªªæ˜ï¼Œè«‹åƒè€ƒ [docs/ocr.md](docs/ocr.md).

## Testing & Linting

- Run `npm run lint` to check for lint issues
- Run `npm run typecheck` to verify TypeScript typings

## E2E / çœŸæ©Ÿé©—è­‰

### App å…§éƒ¨é©—è­‰

- å‰å¾€ **Settings â†’ Speech Debug / QA**ï¼Œå¯æ–¼ `SpeechTest` é å•Ÿå‹•èªéŸ³éŒ„è£½ã€æª¢è¦–æœ€æ–°ç­–ç•¥å»ºè­°èˆ‡ `speech_pipeline_complete` Telemetryã€‚
- é é¢æœƒå³æ™‚åˆ—å‡ºæœ€è¿‘ 25 ç­† telemetry payloadã€æœ€å¾Œä¸€æ¬¡é€å‡ºçš„ `/api/speech-endpoint` è«‹æ±‚èˆ‡å›è¦†ï¼Œæ–¹ä¾¿ QA èˆ‡é™¤éŒ¯ã€‚

### æœ¬æ©Ÿè‡ªå‹•åŒ–è…³æœ¬

1. éœ€ä¸€æ¬¡æˆæ¬Š iOS æ¨¡æ“¬å™¨çš„éº¥å…‹é¢¨ / èªéŸ³è¾¨è­˜æ¬Šé™ï¼ˆmacOSï¼‰ï¼š

   ```bash
   xcrun simctl privacy booted microphone grant com.dealmaster
   xcrun simctl privacy booted speech-recognition grant com.dealmaster
   ```

2. åŸ·è¡Œç«¯å°ç«¯é©—è­‰è…³æœ¬ï¼ˆæœƒå•Ÿå‹•å…§å»º mock serverã€å…ˆä»¥ `scripts/make-sample-wav.js` ç”¢ç”Ÿ `tmp/sample-command.wav`ï¼Œå†é¤µå…¥è©²æª”æ¡ˆä¸¦è§£æå›è¦†ï¼telemetryï¼‰ï¼š

   ```bash
   bash scripts/e2e-local.sh
   ```

   æŒ‡ä»¤çµæŸæ™‚æ‡‰è¼¸å‡ºï¼š

   ```
   ASR final text: ...
   Strategy: ... , EmotionScore: ...
   Telemetry: speech_pipeline_complete total_duration=...ms endpoint_latency=...ms
   ```

3. ç›¸é—œç´€éŒ„ï¼ˆè«‹æ±‚ã€å›æ‡‰ã€telemetryã€ä¼ºæœå™¨æ—¥èªŒï¼‰æœƒå¯«å…¥ `artifacts/e2e-local/` ä¾›å¾ŒçºŒæª¢è¦–ã€‚

### CI Smoke æ¸¬è©¦

- GitHub Actions æ–¼ Pull Request ä¸­æœƒè§¸ç™¼ `e2e-smoke` jobï¼Œä½¿ç”¨ `scripts/ci-e2e-smoke.sh` è·‘ä¸€æ¬¡æœ€çŸ­è·¯å¾‘é©—è­‰ä¸¦ä¸Šå‚³ `artifacts/e2e-smoke/`ã€‚
- å¤±æ•—æ™‚ job æœƒå›å‚³éé›¶ç‹€æ…‹ä¸¦é¡¯ç¤º mock ä¼ºæœå™¨è¼¸å‡ºï¼Œå”åŠ©å®šä½ç«¯å°ç«¯æ•´åˆå•é¡Œã€‚

## License

MIT
